package functioncall

import (
	"context"
	"encoding/json"
	"fmt"
	"io"
	"net/http"
	"regexp"
	"strings"
	"time"

	"ai-api-gateway/internal/infrastructure/logger"

	md "github.com/JohannesKaufmann/html-to-markdown"
)

// SearchService æœç´¢æœåŠ¡æ¥å£
type SearchService interface {
	Search(ctx context.Context, query string) (string, error)
	SearchNews(ctx context.Context, query string) (string, error)
	CrawlURL(ctx context.Context, url string) (string, error)
}

// SearchConfig æœç´¢æœåŠ¡é…ç½®
type SearchConfig struct {
	Service        string `json:"service"`          // æœç´¢æœåŠ¡ç±»å‹
	MaxResults     int    `json:"max_results"`      // æœ€å¤§ç»“æœæ•°
	CrawlResults   int    `json:"crawl_results"`    // æ·±åº¦æœç´¢æ•°é‡
	CrawlContent   bool   `json:"crawl_content"`    // æ˜¯å¦çˆ¬å–ç½‘é¡µå†…å®¹å¹¶è½¬æ¢ä¸ºMarkdown
	Search1APIKey  string `json:"search1api_key"`   // Search1APIå¯†é’¥
	GoogleCX       string `json:"google_cx"`        // Googleè‡ªå®šä¹‰æœç´¢å¼•æ“ID
	GoogleKey      string `json:"google_key"`       // Google APIå¯†é’¥
	BingKey        string `json:"bing_key"`         // Bingæœç´¢APIå¯†é’¥
	SerpAPIKey     string `json:"serpapi_key"`      // SerpAPIå¯†é’¥
	SerperKey      string `json:"serper_key"`       // Serperå¯†é’¥
	SearXNGBaseURL string `json:"searxng_base_url"` // SearXNGæœåŠ¡åœ°å€
}

// SearchResult æœç´¢ç»“æœ
type SearchResult struct {
	Title   string `json:"title"`
	Link    string `json:"link"`
	Snippet string `json:"snippet"`
	Content string `json:"content,omitempty"` // ç½‘é¡µå†…å®¹ï¼ˆMarkdownæ ¼å¼ï¼‰
}

// SearchResponse æœç´¢å“åº”
type SearchResponse struct {
	Results []SearchResult `json:"results"`
}

// CrawlResponse çˆ¬å–å“åº”
type CrawlResponse struct {
	URL     string `json:"url"`
	Title   string `json:"title"`
	Content string `json:"content"`
}

// searchServiceImpl æœç´¢æœåŠ¡å®ç°
type searchServiceImpl struct {
	config     *SearchConfig
	logger     logger.Logger
	httpClient *http.Client
}

// NewSearchService åˆ›å»ºæœç´¢æœåŠ¡
func NewSearchService(config *SearchConfig, logger logger.Logger) SearchService {
	return &searchServiceImpl{
		config: config,
		logger: logger,
		httpClient: &http.Client{
			Timeout: 30 * time.Second,
		},
	}
}

// Search æ‰§è¡Œæœç´¢
func (s *searchServiceImpl) Search(ctx context.Context, query string) (string, error) {
	s.logger.WithFields(map[string]interface{}{
		"service": s.config.Service,
		"query":   query,
		"config":  fmt.Sprintf("%+v", s.config),
	}).Info("Executing search")

	var results []SearchResult
	var err error

	switch s.config.Service {
	case "search1api":
		s.logger.Info("Using Search1API service")
		results, err = s.searchWithSearch1API(ctx, query, false)
	case "google":
		keyLength := len(s.config.GoogleKey)
		if keyLength > 10 {
			keyLength = 10
		}
		s.logger.WithFields(map[string]interface{}{
			"google_cx":  s.config.GoogleCX,
			"google_key": s.config.GoogleKey[:keyLength] + "...",
		}).Info("Using Google Custom Search service")
		results, err = s.searchWithGoogle(ctx, query, false)
	case "bing":
		s.logger.Info("Using Bing Search service")
		results, err = s.searchWithBing(ctx, query, false)
	case "serpapi":
		s.logger.Info("Using SerpAPI service")
		results, err = s.searchWithSerpAPI(ctx, query, false)
	case "serper":
		s.logger.Info("Using Serper service")
		results, err = s.searchWithSerper(ctx, query, false)
	case "duckduckgo":
		s.logger.Info("Using DuckDuckGo service")
		results, err = s.searchWithDuckDuckGo(ctx, query, false)
	case "searxng":
		s.logger.Info("Using SearXNG service")
		results, err = s.searchWithSearXNG(ctx, query, false)
	default:
		s.logger.WithFields(map[string]interface{}{
			"service": s.config.Service,
		}).Error("Unsupported search service")
		return "", fmt.Errorf("unsupported search service: %s", s.config.Service)
	}

	if err != nil {
		s.logger.WithFields(map[string]interface{}{
			"service": s.config.Service,
			"query":   query,
			"error":   err.Error(),
		}).Error("Search failed")
		return "", err
	}

	s.logger.WithFields(map[string]interface{}{
		"service":      s.config.Service,
		"query":        query,
		"result_count": len(results),
		"results":      results,
	}).Info("Search results obtained")

	// å¦‚æœå¯ç”¨äº†å†…å®¹çˆ¬å–ï¼Œåˆ™å¯¹æ¯ä¸ªæœç´¢ç»“æœè¿›è¡Œå†…å®¹æŠ“å–
	if s.config.CrawlContent {
		s.logger.WithFields(map[string]interface{}{
			"result_count": len(results),
		}).Info("Starting content crawling for search results")

		for i := range results {
			if results[i].Link != "" {
				s.logger.WithFields(map[string]interface{}{
					"url":   results[i].Link,
					"title": results[i].Title,
					"index": i + 1,
				}).Info("Crawling content for search result")

				content, err := s.crawlAndConvertToMarkdown(ctx, results[i].Link)
				if err != nil {
					s.logger.WithFields(map[string]interface{}{
						"url":   results[i].Link,
						"error": err.Error(),
						"index": i + 1,
					}).Warn("Failed to crawl content for search result, continuing with next")
					// ç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ªç»“æœï¼Œä¸å› ä¸ºå•ä¸ªå¤±è´¥è€Œä¸­æ–­æ•´ä¸ªæœç´¢
					continue
				}

				// å°†æ¸…ç†åçš„å†…å®¹æ·»åŠ åˆ°æœç´¢ç»“æœä¸­
				cleanedContent := s.summarizeContent(content, 2000) // é™åˆ¶æ¯ä¸ªç»“æœçš„å†…å®¹é•¿åº¦
				results[i].Content = cleanedContent
				s.logger.WithFields(map[string]interface{}{
					"url":             results[i].Link,
					"content_length":  len(cleanedContent),
					"original_length": len(content),
					"index":           i + 1,
				}).Info("Successfully crawled and processed content")
			}
		}

		s.logger.WithFields(map[string]interface{}{
			"result_count": len(results),
		}).Info("Content crawling completed for all search results")
	}

	// æ ¼å¼åŒ–æœç´¢ç»“æœä¸ºç»“æ„åŒ–æ–‡æœ¬
	formattedResults := s.formatSearchResults(query, results)

	s.logger.WithFields(map[string]interface{}{
		"service":         s.config.Service,
		"query":           query,
		"result_count":    len(results),
		"response_length": len(formattedResults),
		"crawl_enabled":   s.config.CrawlContent,
	}).Info("Search completed successfully")

	return formattedResults, nil
}

// SearchNews æ‰§è¡Œæ–°é—»æœç´¢
func (s *searchServiceImpl) SearchNews(ctx context.Context, query string) (string, error) {
	s.logger.WithFields(map[string]interface{}{
		"service": s.config.Service,
		"query":   query,
	}).Info("Executing news search")

	var results []SearchResult
	var err error

	switch s.config.Service {
	case "search1api":
		results, err = s.searchWithSearch1API(ctx, query, true)
	case "google":
		results, err = s.searchWithGoogle(ctx, query, true)
	case "bing":
		results, err = s.searchWithBing(ctx, query, true)
	case "serpapi":
		results, err = s.searchWithSerpAPI(ctx, query, true)
	case "serper":
		results, err = s.searchWithSerper(ctx, query, true)
	case "duckduckgo":
		results, err = s.searchWithDuckDuckGo(ctx, query, true)
	case "searxng":
		results, err = s.searchWithSearXNG(ctx, query, true)
	default:
		return "", fmt.Errorf("unsupported search service: %s", s.config.Service)
	}

	if err != nil {
		s.logger.WithFields(map[string]interface{}{
			"service": s.config.Service,
			"query":   query,
			"error":   err.Error(),
		}).Error("News search failed")
		return "", err
	}

	// å¦‚æœå¯ç”¨äº†å†…å®¹çˆ¬å–ï¼Œåˆ™å¯¹æ¯ä¸ªæ–°é—»ç»“æœè¿›è¡Œå†…å®¹æŠ“å–
	if s.config.CrawlContent {
		s.logger.WithFields(map[string]interface{}{
			"result_count": len(results),
		}).Info("Starting content crawling for news results")

		for i := range results {
			if results[i].Link != "" {
				s.logger.WithFields(map[string]interface{}{
					"url":   results[i].Link,
					"title": results[i].Title,
					"index": i + 1,
				}).Info("Crawling content for news result")

				content, err := s.crawlAndConvertToMarkdown(ctx, results[i].Link)
				if err != nil {
					s.logger.WithFields(map[string]interface{}{
						"url":   results[i].Link,
						"error": err.Error(),
						"index": i + 1,
					}).Warn("Failed to crawl content for news result, continuing with next")
					// ç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ªç»“æœï¼Œä¸å› ä¸ºå•ä¸ªå¤±è´¥è€Œä¸­æ–­æ•´ä¸ªæœç´¢
					continue
				}

				// å°†æ¸…ç†åçš„å†…å®¹æ·»åŠ åˆ°æ–°é—»ç»“æœä¸­
				cleanedContent := s.summarizeContent(content, 2000) // é™åˆ¶æ¯ä¸ªç»“æœçš„å†…å®¹é•¿åº¦
				results[i].Content = cleanedContent
				s.logger.WithFields(map[string]interface{}{
					"url":             results[i].Link,
					"content_length":  len(cleanedContent),
					"original_length": len(content),
					"index":           i + 1,
				}).Info("Successfully crawled and processed news content")
			}
		}

		s.logger.WithFields(map[string]interface{}{
			"result_count": len(results),
		}).Info("Content crawling completed for all news results")
	}

	// æ ¼å¼åŒ–æ–°é—»æœç´¢ç»“æœä¸ºç»“æ„åŒ–æ–‡æœ¬
	formattedResults := s.formatSearchResults("æ–°é—»æœç´¢: "+query, results)

	s.logger.WithFields(map[string]interface{}{
		"service":         s.config.Service,
		"query":           query,
		"result_count":    len(results),
		"response_length": len(formattedResults),
		"crawl_enabled":   s.config.CrawlContent,
	}).Info("News search completed successfully")

	return formattedResults, nil
}

// CrawlURL çˆ¬å–ç½‘é¡µå†…å®¹
func (s *searchServiceImpl) CrawlURL(ctx context.Context, targetURL string) (string, error) {
	s.logger.WithFields(map[string]interface{}{
		"url": targetURL,
	}).Info("Crawling URL")

	// ä½¿ç”¨é€šç”¨çš„çˆ¬å–æœåŠ¡
	crawlURL := "https://crawl.search1api.com"

	requestBody := map[string]interface{}{
		"url": targetURL,
	}

	requestJSON, err := json.Marshal(requestBody)
	if err != nil {
		return "", fmt.Errorf("failed to marshal crawl request: %w", err)
	}

	req, err := http.NewRequestWithContext(ctx, "POST", crawlURL, strings.NewReader(string(requestJSON)))
	if err != nil {
		return "", fmt.Errorf("failed to create crawl request: %w", err)
	}

	req.Header.Set("Content-Type", "application/json")

	resp, err := s.httpClient.Do(req)
	if err != nil {
		return "", fmt.Errorf("failed to execute crawl request: %w", err)
	}
	defer resp.Body.Close()

	if resp.StatusCode != http.StatusOK {
		return "", fmt.Errorf("crawl request failed with status: %d", resp.StatusCode)
	}

	var crawlResponse CrawlResponse
	if err := json.NewDecoder(resp.Body).Decode(&crawlResponse); err != nil {
		return "", fmt.Errorf("failed to decode crawl response: %w", err)
	}

	responseJSON, err := json.Marshal(crawlResponse)
	if err != nil {
		return "", fmt.Errorf("failed to marshal crawl response: %w", err)
	}

	s.logger.WithFields(map[string]interface{}{
		"url":   targetURL,
		"title": crawlResponse.Title,
	}).Info("URL crawling completed successfully")

	return string(responseJSON), nil
}

// crawlAndConvertToMarkdown çˆ¬å–ç½‘é¡µå†…å®¹å¹¶è½¬æ¢ä¸ºMarkdown
func (s *searchServiceImpl) crawlAndConvertToMarkdown(ctx context.Context, url string) (string, error) {
	s.logger.WithFields(map[string]interface{}{
		"url": url,
	}).Info("Crawling URL and converting to Markdown")

	// åˆ›å»ºHTTPè¯·æ±‚
	req, err := http.NewRequestWithContext(ctx, "GET", url, nil)
	if err != nil {
		s.logger.WithFields(map[string]interface{}{
			"error": err.Error(),
			"url":   url,
		}).Error("Failed to create HTTP request")
		return "", fmt.Errorf("failed to create request: %w", err)
	}

	// è®¾ç½®User-Agenté¿å…è¢«åçˆ¬è™«
	req.Header.Set("User-Agent", "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36")

	// æ‰§è¡Œè¯·æ±‚
	resp, err := s.httpClient.Do(req)
	if err != nil {
		s.logger.WithFields(map[string]interface{}{
			"error": err.Error(),
			"url":   url,
		}).Error("Failed to execute HTTP request")
		return "", fmt.Errorf("failed to execute request: %w", err)
	}
	defer resp.Body.Close()

	// æ£€æŸ¥å“åº”çŠ¶æ€
	if resp.StatusCode != http.StatusOK {
		s.logger.WithFields(map[string]interface{}{
			"status_code": resp.StatusCode,
			"url":         url,
		}).Error("HTTP request failed with non-200 status")
		return "", fmt.Errorf("HTTP request failed with status: %d", resp.StatusCode)
	}

	// è¯»å–å“åº”å†…å®¹
	htmlContent, err := io.ReadAll(resp.Body)
	if err != nil {
		s.logger.WithFields(map[string]interface{}{
			"error": err.Error(),
			"url":   url,
		}).Error("Failed to read response body")
		return "", fmt.Errorf("failed to read response: %w", err)
	}

	// è½¬æ¢HTMLåˆ°Markdown
	converter := md.NewConverter("", true, nil)
	markdown, err := converter.ConvertString(string(htmlContent))
	if err != nil {
		s.logger.WithFields(map[string]interface{}{
			"error": err.Error(),
			"url":   url,
		}).Error("Failed to convert HTML to Markdown")
		return "", fmt.Errorf("failed to convert HTML to Markdown: %w", err)
	}

	s.logger.WithFields(map[string]interface{}{
		"url":             url,
		"content_length":  len(markdown),
		"original_length": len(htmlContent),
	}).Info("Successfully converted HTML to Markdown")

	return markdown, nil
}

// cleanMarkdownContent æ¸…ç†Markdownå†…å®¹ï¼Œå»é™¤æ— å…³éƒ¨åˆ†
func (s *searchServiceImpl) cleanMarkdownContent(content string) string {
	// å»é™¤å¸¸è§çš„å¯¼èˆªå’Œé¡µè„šå†…å®¹
	lines := strings.Split(content, "\n")
	var cleanedLines []string

	// æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…å¸¸è§çš„æ— å…³å†…å®¹
	skipPatterns := []*regexp.Regexp{
		regexp.MustCompile(`(?i)^(menu|navigation|nav|header|footer|sidebar)`),
		regexp.MustCompile(`(?i)(cookie|privacy|terms|contact|about us|subscribe|newsletter)`),
		regexp.MustCompile(`(?i)^(home|back to top|skip to|jump to)`),
		regexp.MustCompile(`^[\s\*\-\+]*$`), // ç©ºè¡Œæˆ–åªæœ‰ç¬¦å·çš„è¡Œ
	}

	for _, line := range lines {
		line = strings.TrimSpace(line)
		if line == "" {
			continue
		}

		// æ£€æŸ¥æ˜¯å¦åº”è¯¥è·³è¿‡è¿™ä¸€è¡Œ
		shouldSkip := false
		for _, pattern := range skipPatterns {
			if pattern.MatchString(line) {
				shouldSkip = true
				break
			}
		}

		if !shouldSkip && len(line) > 10 { // åªä¿ç•™æœ‰æ„ä¹‰çš„å†…å®¹
			cleanedLines = append(cleanedLines, line)
		}
	}

	return strings.Join(cleanedLines, "\n")
}

// summarizeContent æ€»ç»“å†…å®¹ï¼Œæå–å…³é”®ä¿¡æ¯
func (s *searchServiceImpl) summarizeContent(content string, maxLength int) string {
	cleaned := s.cleanMarkdownContent(content)

	if len(cleaned) <= maxLength {
		return cleaned
	}

	// æŒ‰æ®µè½åˆ†å‰²
	paragraphs := strings.Split(cleaned, "\n")
	var result []string
	currentLength := 0

	for _, paragraph := range paragraphs {
		if currentLength+len(paragraph) > maxLength {
			break
		}
		if len(paragraph) > 20 { // åªä¿ç•™æœ‰æ„ä¹‰çš„æ®µè½
			result = append(result, paragraph)
			currentLength += len(paragraph) + 1
		}
	}

	summary := strings.Join(result, "\n")
	if len(summary) < len(cleaned) {
		summary += "\n\n[å†…å®¹å·²æˆªæ–­...]"
	}

	return summary
}

// formatSearchResults æ ¼å¼åŒ–æœç´¢ç»“æœä¸ºç»“æ„åŒ–æ–‡æœ¬
func (s *searchServiceImpl) formatSearchResults(query string, results []SearchResult) string {
	var output strings.Builder

	if len(results) == 0 {
		output.WriteString("æœªæ‰¾åˆ°ç›¸å…³ç»“æœã€‚\n")
		return output.String()
	}

	// æå–å’Œæ•´åˆå…³é”®ä¿¡æ¯
	keyPoints := s.extractKeyPointsWithSources(results)

	// ç”Ÿæˆç»¼åˆå›ç­”
	output.WriteString(fmt.Sprintf("æ ¹æ®æœç´¢æŸ¥è¯¢ã€Œ%sã€ï¼Œä¸ºæ‚¨æ•´ç†äº†ä»¥ä¸‹ä¿¡æ¯ï¼š\n\n", query))

	// ä¸»è¦å†…å®¹éƒ¨åˆ† - åŒ…å«æ¥æºå¼•ç”¨
	if len(keyPoints) > 0 {
		for i, point := range keyPoints {
			output.WriteString(fmt.Sprintf("%d. **%s**ï¼š%s", i+1, point.Category, point.Content))
			// æ·»åŠ æ¥æºå¼•ç”¨
			if len(point.Sources) > 0 {
				output.WriteString(" ")
				for j, source := range point.Sources {
					if j > 0 {
						output.WriteString(", ")
					}
					output.WriteString(fmt.Sprintf("[[%d]]", source.Index))
				}
			}
			output.WriteString("\n\n")
		}
	}

	// è¯¦ç»†æ¥æºä¿¡æ¯
	output.WriteString("## å‚è€ƒæ¥æº\n\n")
	output.WriteString("ä»¥ä¸‹æ˜¯æœ¬æ¬¡æœç´¢çš„è¯¦ç»†æ¥æºä¿¡æ¯ï¼š\n\n")

	for i, result := range results {
		output.WriteString(fmt.Sprintf("**[%d]** %s\n", i+1, result.Title))
		output.WriteString(fmt.Sprintf("ğŸ”— %s\n", result.Link))
		if result.Snippet != "" {
			output.WriteString(fmt.Sprintf("ğŸ“ %s\n", result.Snippet))
		}
		output.WriteString("\n")
	}

	return output.String()
}

// KeyPoint å…³é”®ä¿¡æ¯ç‚¹
type KeyPoint struct {
	Category string      // åˆ†ç±»
	Content  string      // å†…å®¹
	Sources  []SourceRef // æ¥æºå¼•ç”¨
}

// SourceRef æ¥æºå¼•ç”¨
type SourceRef struct {
	Index int    // æ¥æºç´¢å¼•
	Title string // æ¥æºæ ‡é¢˜
	URL   string // æ¥æºURL
}

// extractKeyPointsWithSources ä»æœç´¢ç»“æœä¸­æå–å…³é”®ä¿¡æ¯ç‚¹ï¼ˆåŒ…å«æ¥æºï¼‰
func (s *searchServiceImpl) extractKeyPointsWithSources(results []SearchResult) []KeyPoint {
	var keyPoints []KeyPoint

	// åˆ†ææ¯ä¸ªæœç´¢ç»“æœï¼Œæå–å…³é”®ä¿¡æ¯
	for i, result := range results {
		sourceRef := SourceRef{
			Index: i + 1,
			Title: result.Title,
			URL:   result.Link,
		}

		if result.Content != "" {
			// æ ¹æ®å†…å®¹é•¿åº¦å’Œè´¨é‡æå–å…³é”®ä¿¡æ¯
			points := s.analyzeContentWithSource(result.Title, result.Content, sourceRef)
			keyPoints = append(keyPoints, points...)
		} else if result.Snippet != "" {
			// å¦‚æœæ²¡æœ‰è¯¦ç»†å†…å®¹ï¼Œä½¿ç”¨æ‘˜è¦
			keyPoints = append(keyPoints, KeyPoint{
				Category: s.categorizeContent(result.Title),
				Content:  result.Snippet,
				Sources:  []SourceRef{sourceRef},
			})
		}
	}

	// å»é‡å’Œåˆå¹¶ç›¸ä¼¼çš„ä¿¡æ¯ç‚¹
	return s.deduplicateKeyPointsWithSources(keyPoints)
}

// extractKeyPoints ä»æœç´¢ç»“æœä¸­æå–å…³é”®ä¿¡æ¯ç‚¹ï¼ˆå…¼å®¹æ€§æ–¹æ³•ï¼‰
func (s *searchServiceImpl) extractKeyPoints(results []SearchResult) []KeyPoint {
	keyPointsWithSources := s.extractKeyPointsWithSources(results)
	// ç§»é™¤æ¥æºä¿¡æ¯ï¼Œä¿æŒå‘åå…¼å®¹
	var keyPoints []KeyPoint
	for _, point := range keyPointsWithSources {
		keyPoints = append(keyPoints, KeyPoint{
			Category: point.Category,
			Content:  point.Content,
			Sources:  nil,
		})
	}
	return keyPoints
}

// analyzeContentWithSource åˆ†æå†…å®¹å¹¶æå–å…³é”®ä¿¡æ¯ï¼ˆåŒ…å«æ¥æºï¼‰
func (s *searchServiceImpl) analyzeContentWithSource(title, content string, source SourceRef) []KeyPoint {
	var points []KeyPoint

	// æŒ‰æ®µè½åˆ†æå†…å®¹
	paragraphs := strings.Split(content, "\n")
	category := s.categorizeContent(title)

	var meaningfulParagraphs []string
	for _, paragraph := range paragraphs {
		paragraph = strings.TrimSpace(paragraph)
		if len(paragraph) > 50 && !s.isBoilerplate(paragraph) {
			meaningfulParagraphs = append(meaningfulParagraphs, paragraph)
		}
	}

	// åˆå¹¶ç›¸å…³æ®µè½
	if len(meaningfulParagraphs) > 0 {
		// å–å‰3ä¸ªæœ€æœ‰æ„ä¹‰çš„æ®µè½
		maxParagraphs := 3
		if len(meaningfulParagraphs) < maxParagraphs {
			maxParagraphs = len(meaningfulParagraphs)
		}

		combinedContent := strings.Join(meaningfulParagraphs[:maxParagraphs], " ")
		if len(combinedContent) > 500 {
			combinedContent = combinedContent[:500] + "..."
		}

		points = append(points, KeyPoint{
			Category: category,
			Content:  combinedContent,
			Sources:  []SourceRef{source},
		})
	}

	return points
}

// analyzeContent åˆ†æå†…å®¹å¹¶æå–å…³é”®ä¿¡æ¯ï¼ˆå…¼å®¹æ€§æ–¹æ³•ï¼‰
func (s *searchServiceImpl) analyzeContent(title, content string) []KeyPoint {
	// åˆ›å»ºä¸€ä¸ªè™šæ‹Ÿçš„æ¥æºå¼•ç”¨
	dummySource := SourceRef{Index: 0, Title: title, URL: ""}
	pointsWithSources := s.analyzeContentWithSource(title, content, dummySource)

	// ç§»é™¤æ¥æºä¿¡æ¯
	var points []KeyPoint
	for _, point := range pointsWithSources {
		points = append(points, KeyPoint{
			Category: point.Category,
			Content:  point.Content,
			Sources:  nil,
		})
	}

	return points
}

// categorizeContent æ ¹æ®æ ‡é¢˜å†…å®¹è¿›è¡Œåˆ†ç±»
func (s *searchServiceImpl) categorizeContent(title string) string {
	title = strings.ToLower(title)

	if strings.Contains(title, "å®šä¹‰") || strings.Contains(title, "ä»€ä¹ˆæ˜¯") || strings.Contains(title, "ä»‹ç»") {
		return "å®šä¹‰è¯´æ˜"
	} else if strings.Contains(title, "ä½¿ç”¨") || strings.Contains(title, "åº”ç”¨") || strings.Contains(title, "åŠŸèƒ½") {
		return "ä½¿ç”¨æ–¹æ³•"
	} else if strings.Contains(title, "ç‰¹ç‚¹") || strings.Contains(title, "ç‰¹æ€§") || strings.Contains(title, "ä¼˜åŠ¿") {
		return "ä¸»è¦ç‰¹ç‚¹"
	} else if strings.Contains(title, "å‘å±•") || strings.Contains(title, "å†å²") || strings.Contains(title, "è¶‹åŠ¿") {
		return "å‘å±•ç°çŠ¶"
	} else if strings.Contains(title, "æŠ€æœ¯") || strings.Contains(title, "åŸç†") || strings.Contains(title, "å®ç°") {
		return "æŠ€æœ¯åŸç†"
	} else {
		return "ç›¸å…³ä¿¡æ¯"
	}
}

// isBoilerplate åˆ¤æ–­æ˜¯å¦ä¸ºæ ·æ¿æ–‡æœ¬ï¼ˆå¯¼èˆªã€å¹¿å‘Šç­‰ï¼‰
func (s *searchServiceImpl) isBoilerplate(text string) bool {
	boilerplatePatterns := []string{
		"cookie", "privacy", "terms", "contact", "subscribe",
		"menu", "navigation", "footer", "header", "sidebar",
		"click here", "read more", "learn more", "sign up",
		"copyright", "all rights reserved", "powered by",
	}

	textLower := strings.ToLower(text)
	for _, pattern := range boilerplatePatterns {
		if strings.Contains(textLower, pattern) {
			return true
		}
	}

	return false
}

// deduplicateKeyPointsWithSources å»é‡å’Œåˆå¹¶ç›¸ä¼¼çš„å…³é”®ä¿¡æ¯ç‚¹ï¼ˆåŒ…å«æ¥æºï¼‰
func (s *searchServiceImpl) deduplicateKeyPointsWithSources(points []KeyPoint) []KeyPoint {
	if len(points) <= 1 {
		return points
	}

	var result []KeyPoint
	categoryMap := make(map[string][]KeyPoint)

	// æŒ‰åˆ†ç±»ç»„ç»‡å†…å®¹
	for _, point := range points {
		categoryMap[point.Category] = append(categoryMap[point.Category], point)
	}

	// åˆå¹¶åŒç±»ä¿¡æ¯
	for category, categoryPoints := range categoryMap {
		if len(categoryPoints) == 1 {
			result = append(result, categoryPoints[0])
		} else {
			// åˆå¹¶å¤šä¸ªå†…å®¹ï¼Œå–æœ€æœ‰ä»·å€¼çš„éƒ¨åˆ†
			var contents []string
			var allSources []SourceRef

			for _, point := range categoryPoints {
				contents = append(contents, point.Content)
				allSources = append(allSources, point.Sources...)
			}

			combinedContent := s.combineContents(contents)
			// å»é‡æ¥æº
			uniqueSources := s.deduplicateSources(allSources)

			result = append(result, KeyPoint{
				Category: category,
				Content:  combinedContent,
				Sources:  uniqueSources,
			})
		}
	}

	return result
}

// deduplicateKeyPoints å»é‡å’Œåˆå¹¶ç›¸ä¼¼çš„å…³é”®ä¿¡æ¯ç‚¹ï¼ˆå…¼å®¹æ€§æ–¹æ³•ï¼‰
func (s *searchServiceImpl) deduplicateKeyPoints(points []KeyPoint) []KeyPoint {
	if len(points) <= 1 {
		return points
	}

	var result []KeyPoint
	categoryMap := make(map[string][]string)

	// æŒ‰åˆ†ç±»ç»„ç»‡å†…å®¹
	for _, point := range points {
		categoryMap[point.Category] = append(categoryMap[point.Category], point.Content)
	}

	// åˆå¹¶åŒç±»ä¿¡æ¯
	for category, contents := range categoryMap {
		if len(contents) == 1 {
			result = append(result, KeyPoint{
				Category: category,
				Content:  contents[0],
				Sources:  nil,
			})
		} else {
			// åˆå¹¶å¤šä¸ªå†…å®¹ï¼Œå–æœ€æœ‰ä»·å€¼çš„éƒ¨åˆ†
			combinedContent := s.combineContents(contents)
			result = append(result, KeyPoint{
				Category: category,
				Content:  combinedContent,
				Sources:  nil,
			})
		}
	}

	return result
}

// deduplicateSources å»é‡æ¥æºå¼•ç”¨
func (s *searchServiceImpl) deduplicateSources(sources []SourceRef) []SourceRef {
	seen := make(map[string]bool)
	var result []SourceRef

	for _, source := range sources {
		key := source.URL
		if !seen[key] {
			seen[key] = true
			result = append(result, source)
		}
	}

	return result
}

// combineContents åˆå¹¶å¤šä¸ªå†…å®¹
func (s *searchServiceImpl) combineContents(contents []string) string {
	if len(contents) == 0 {
		return ""
	}

	// é€‰æ‹©æœ€é•¿ä¸”æœ€æœ‰ä¿¡æ¯é‡çš„å†…å®¹ä½œä¸ºä¸»è¦å†…å®¹
	var bestContent string
	maxScore := 0

	for _, content := range contents {
		score := len(content) // ç®€å•çš„è¯„åˆ†æœºåˆ¶ï¼Œå¯ä»¥åç»­ä¼˜åŒ–
		if score > maxScore {
			maxScore = score
			bestContent = content
		}
	}

	return bestContent
}
